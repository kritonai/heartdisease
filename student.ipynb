{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kritonai/heartdisease/blob/main/student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-1",
      "metadata": {
        "id": "intro-1"
      },
      "source": [
        "# **Heart Failure Prediction - Model Training & Evaluation**\n",
        "\n",
        "## Let's Build Some Models! üöÄ\n",
        "\n",
        "In this notebook, we'll:\n",
        "- Split our data into training and test sets\n",
        "- Train multiple machine learning models\n",
        "- Evaluate and compare their performance\n",
        "- Select the best model for heart disease prediction\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*cG6U1qstYDijh9bPL42e-Q.jpeg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-header",
      "metadata": {
        "id": "imports-header"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-cell",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports-cell",
        "outputId": "62f1eaaa-aa64-484f-e51b-199961eacfc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully! ‚úì\n"
          ]
        }
      ],
      "source": [
        "# Data handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_curve, auc,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Set style\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully! ‚úì\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Suppose df_label is the dataframe at the end of your data preprocessing step."
      ],
      "metadata": {
        "id": "eQDUuwACrSy3"
      },
      "id": "eQDUuwACrSy3"
    },
    {
      "cell_type": "markdown",
      "id": "train-test-split-intro",
      "metadata": {
        "id": "train-test-split-intro"
      },
      "source": [
        "# Train-Test Split: Why Do We Need It?\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*-8_kogvwmL1H6ooN1A1tsQ.png\" width=\"600\">\n",
        "\n",
        "## The Problem\n",
        "If we train and test on the same data, our model will **memorize** the answers instead of **learning patterns**. This is like:\n",
        "- üìö Memorizing exam questions instead of understanding concepts\n",
        "- üéÆ Playing a game level you've already seen vs. a new level\n",
        "\n",
        "## The Solution: Train-Test Split\n",
        "\n",
        "We split our data into two parts:\n",
        "\n",
        "### Training Set (typically 70-80%)\n",
        "- Used to **train** the model\n",
        "- Model learns patterns from this data\n",
        "- Model **sees the answers** here\n",
        "\n",
        "### Test Set (typically 20-30%)\n",
        "- Used to **evaluate** the model\n",
        "- Model has **never seen** this data before\n",
        "- Tells us how well the model generalizes to new patients\n",
        "\n",
        "## Important Rules üö®\n",
        "1. **Never train on test data** - That's cheating!\n",
        "2. **Split BEFORE any preprocessing** that uses statistics (like scaling)\n",
        "3. **Use the same split** for fair comparison between models\n",
        "4. **Stratify** when classes are imbalanced (keeps the same proportion in both sets)\n",
        "\n",
        "## How It Works\n",
        "```python\n",
        "train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "- `test_size=0.2`: Use 20% for testing, 80% for training\n",
        "- `random_state=42`: Makes the split reproducible (same split every time)\n",
        "- `stratify=y`: Keeps class proportions balanced in both sets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-for-trees",
      "metadata": {
        "id": "split-for-trees"
      },
      "source": [
        "## Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split-label",
      "metadata": {
        "id": "split-label"
      },
      "outputs": [],
      "source": [
        "# Split your dataset into training and testing subsets. Use a test size of 20%. Research what \"stratify\" is and decide if you should implement it in your case\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-intro",
      "metadata": {
        "id": "scaling-intro"
      },
      "source": [
        "# Feature Scaling: Making Features Comparable\n",
        "\n",
        "\n",
        "## Why Scale?\n",
        "\n",
        "Imagine you're comparing patients:\n",
        "- Age: 28-77 years\n",
        "- Cholesterol: 100-600 mg/dl\n",
        "- Oldpeak: 0-6.2\n",
        "\n",
        "Without scaling, Cholesterol dominates because its numbers are bigger!\n",
        "\n",
        "## Which Models Need Scaling?\n",
        "\n",
        "### ‚úÖ Need Scaling:\n",
        "- **KNN**: Uses distances between points\n",
        "- **SVM**: Uses distances to find decision boundary\n",
        "- **Logistic Regression**: Gradient descent works better with scaled features\n",
        "- **Naive Bayes**: Can benefit from scaling in some implementations\n",
        "\n",
        "### ‚ùå Don't Need Scaling:\n",
        "- **Decision Trees**: Split on thresholds, not distances\n",
        "- **Random Forest**: Ensemble of decision trees\n",
        "\n",
        "## Important: Scale AFTER Splitting! üö®\n",
        "\n",
        "```python\n",
        "# ‚ùå WRONG - Information leakage!\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test = train_test_split(X_scaled)\n",
        "\n",
        "# ‚úÖ CORRECT - No leakage\n",
        "X_train, X_test = train_test_split(X)\n",
        "scaler.fit(X_train)  # Learn only from training\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "scaling-implementation",
      "metadata": {
        "id": "scaling-implementation"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Identify numerical columns that need scaling\n",
        "\n",
        "# Initialize the scaler\n",
        "\n",
        "# Fit on training data ONLY\n",
        "\n",
        "# Transform both train and test\n",
        "\n",
        "# Print the numerical columns that you scales before and after the scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "models-intro",
      "metadata": {
        "id": "models-intro"
      },
      "source": [
        "# Machine Learning Models Overview\n",
        "\n",
        "You'll train 6 different models. Each has different strengths!\n",
        "\n",
        "| Model | Type | Best For | Key Strength |\n",
        "|-------|------|----------|-------------|\n",
        "| **Logistic Regression** | Linear | Baseline, interpretability | Simple, fast, probabilistic |\n",
        "| **Decision Tree** | Tree | Non-linear patterns | Easy to visualize and interpret |\n",
        "| **Random Forest** | Ensemble | Robust predictions | Reduces overfitting, handles complexity |\n",
        "| **Naive Bayes** | Probabilistic | Small datasets | Fast, works with high dimensions |\n",
        "| **SVM** | Margin-based | Complex boundaries | Powerful for non-linear problems |\n",
        "| **KNN** | Instance-based | Local patterns | Simple, no training phase |\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*m6kKsW0O-wWH8Xg_ZA3v8w.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baseline-intro",
      "metadata": {
        "id": "baseline-intro"
      },
      "source": [
        "# 1Ô∏è‚É£ Logistic Regression (Baseline Model)\n",
        "\n",
        "## What is it?\n",
        "Despite its name, Logistic Regression is used for **classification**, not regression!\n",
        "\n",
        "## How it works:\n",
        "- Draws a **linear decision boundary** between classes\n",
        "- Outputs **probabilities** (0 to 1) instead of just class labels\n",
        "- Uses a sigmoid function to squash predictions between 0 and 1\n",
        "\n",
        "## Why start here?\n",
        "- ‚úÖ Simple and fast\n",
        "- ‚úÖ Good baseline to beat\n",
        "- ‚úÖ Provides probability estimates\n",
        "- ‚úÖ Interpretable coefficients\n",
        "\n",
        "## When to use:\n",
        "- Linear relationships between features\n",
        "- Need probability scores\n",
        "- Want to understand feature importance\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/1*dm6ZaX5fuSmuVvM4Ds-vcg.gif\" width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "logistic-regression",
      "metadata": {
        "id": "logistic-regression"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train_scaled, y_train_onehot)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_log = log_reg.predict(X_test_scaled)\n",
        "y_pred_proba_log = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_log = accuracy_score(y_test_onehot, y_pred_log)\n",
        "\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Accuracy: {accuracy_log:.4f} ({accuracy_log*100:.2f}%)\")\n",
        "print(\"\\nFirst 10 predictions vs actual:\")\n",
        "comparison = pd.DataFrame({\n",
        "    'Actual': y_test_onehot.values[:10],\n",
        "    'Predicted': y_pred_log[:10],\n",
        "    'Probability': y_pred_proba_log[:10]\n",
        "})\n",
        "print(comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "decision-tree-intro",
      "metadata": {
        "id": "decision-tree-intro"
      },
      "source": [
        "# 2Ô∏è‚É£ Decision Tree\n",
        "\n",
        "## What is it?\n",
        "A tree of yes/no questions that leads to a prediction!\n",
        "\n",
        "## How it works:\n",
        "```\n",
        "Is Oldpeak > 1.0?\n",
        "‚îú‚îÄ‚îÄ Yes ‚Üí Is MaxHR < 140?\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Yes ‚Üí Heart Disease ‚ù§Ô∏è\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ No ‚Üí Healthy ‚úÖ\n",
        "‚îî‚îÄ‚îÄ No ‚Üí Healthy ‚úÖ\n",
        "```\n",
        "\n",
        "## Advantages:\n",
        "- ‚úÖ Very interpretable (you can visualize the tree!)\n",
        "- ‚úÖ Handles non-linear relationships\n",
        "- ‚úÖ No feature scaling needed\n",
        "- ‚úÖ Can capture complex interactions\n",
        "\n",
        "## Disadvantages:\n",
        "- ‚ùå Prone to overfitting (memorizing training data)\n",
        "- ‚ùå Unstable (small data changes = different tree)\n",
        "- ‚ùå Can create overly complex trees\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1170/1*XMId5sJqPtm8-RIwVVz2tg.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decision-tree",
      "metadata": {
        "id": "decision-tree"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "random-forest-intro",
      "metadata": {
        "id": "random-forest-intro"
      },
      "source": [
        "# 3Ô∏è‚É£ Random Forest\n",
        "\n",
        "## What is it?\n",
        "**\"Wisdom of the Crowd\"** - Many decision trees voting together!\n",
        "\n",
        "## How it works:\n",
        "1. Create 100 different decision trees (each sees random subset of data)\n",
        "2. Each tree makes a prediction\n",
        "3. Take a **majority vote** (most common prediction wins)\n",
        "\n",
        "```\n",
        "Tree 1: Heart Disease ‚ù§Ô∏è\n",
        "Tree 2: Heart Disease ‚ù§Ô∏è  \n",
        "Tree 3: Healthy ‚úÖ         } ‚Üí Final: Heart Disease ‚ù§Ô∏è\n",
        "Tree 4: Heart Disease ‚ù§Ô∏è  \n",
        "...     \n",
        "Tree 100: Heart Disease ‚ù§Ô∏è\n",
        "```\n",
        "\n",
        "## Why it's powerful:\n",
        "- ‚úÖ Reduces overfitting (trees average out their mistakes)\n",
        "- ‚úÖ More stable than single decision tree\n",
        "- ‚úÖ Handles complex patterns\n",
        "- ‚úÖ Provides feature importance\n",
        "- ‚úÖ One of the best \"out-of-the-box\" models\n",
        "\n",
        "## Trade-offs:\n",
        "- ‚ùå Less interpretable (can't visualize 100 trees easily)\n",
        "- ‚ùå Slower to train and predict\n",
        "- ‚ùå Needs more memory\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1092/1*i0o8mjFfCn-uD79-F1Cqkw.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-forest",
      "metadata": {
        "id": "random-forest"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "naive-bayes-intro",
      "metadata": {
        "id": "naive-bayes-intro"
      },
      "source": [
        "# 4Ô∏è‚É£ Naive Bayes\n",
        "\n",
        "## What is it?\n",
        "A probabilistic model based on Bayes' Theorem. It calculates the probability of a patient having heart disease given their features.\n",
        "\n",
        "## How it works:\n",
        "For each patient, it calculates:\n",
        "```\n",
        "P(Heart Disease | Features) = P(Features | Heart Disease) √ó P(Heart Disease) / P(Features)\n",
        "```\n",
        "\n",
        "## The \"Naive\" Part:\n",
        "Assumes all features are **independent** (they don't influence each other)\n",
        "- Example: Assumes Age and Cholesterol are independent\n",
        "- This is often false but works surprisingly well anyway!\n",
        "\n",
        "## Advantages:\n",
        "- ‚úÖ Very fast to train and predict\n",
        "- ‚úÖ Works well with small datasets\n",
        "- ‚úÖ Good with high-dimensional data\n",
        "- ‚úÖ Provides probability estimates\n",
        "\n",
        "## When to use:\n",
        "- Small training datasets\n",
        "- Need fast predictions\n",
        "- Text classification (spam detection, sentiment analysis)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*tjcmj9cDQ-rHXAtxCu5bRQ.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "naive-bayes",
      "metadata": {
        "id": "naive-bayes"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svm-intro",
      "metadata": {
        "id": "svm-intro"
      },
      "source": [
        "# 5Ô∏è‚É£ Support Vector Machine (SVM)\n",
        "\n",
        "## What is it?\n",
        "Finds the **best line** (or hyperplane) that separates the two classes with the **maximum margin**.\n",
        "\n",
        "## How it works:\n",
        "1. Find the line that separates classes\n",
        "2. Maximize the distance to the nearest points from each class\n",
        "3. These nearest points are called **support vectors**\n",
        "\n",
        "## The Kernel Trick üé©:\n",
        "Can handle non-linear boundaries using kernels:\n",
        "- **Linear**: Straight line separation\n",
        "- **RBF (Radial Basis Function)**: Curved, complex boundaries\n",
        "- **Polynomial**: Polynomial curves\n",
        "\n",
        "## Advantages:\n",
        "- ‚úÖ Effective in high-dimensional spaces\n",
        "- ‚úÖ Works well with clear margin of separation\n",
        "- ‚úÖ Versatile (different kernels for different problems)\n",
        "- ‚úÖ Robust to overfitting (especially with RBF kernel)\n",
        "\n",
        "## Disadvantages:\n",
        "- ‚ùå Slow with large datasets (>10,000 samples)\n",
        "- ‚ùå Sensitive to feature scaling (must scale!)\n",
        "- ‚ùå Hard to interpret\n",
        "- ‚ùå Needs tuning of hyperparameters (C, gamma)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*ZpkLQf2FNfzfH4HXeMw4MQ.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svm",
      "metadata": {
        "id": "svm"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knn-intro",
      "metadata": {
        "id": "knn-intro"
      },
      "source": [
        "# 6Ô∏è‚É£ K-Nearest Neighbors (KNN)\n",
        "\n",
        "## What is it?\n",
        "**\"You are the average of your 5 closest friends\"** - looks at nearby data points to make predictions!\n",
        "\n",
        "## How it works:\n",
        "For a new patient:\n",
        "1. Find the K nearest neighbors (patients with similar features)\n",
        "2. Look at their diagnoses (heart disease or healthy)\n",
        "3. Take a **majority vote**\n",
        "\n",
        "```\n",
        "New Patient: ?\n",
        "\n",
        "5 Nearest Neighbors:\n",
        "1. Heart Disease ‚ù§Ô∏è\n",
        "2. Heart Disease ‚ù§Ô∏è\n",
        "3. Healthy ‚úÖ          } ‚Üí Prediction: Heart Disease ‚ù§Ô∏è\n",
        "4. Heart Disease ‚ù§Ô∏è   (3 votes vs 2 votes)\n",
        "5. Healthy ‚úÖ\n",
        "```\n",
        "\n",
        "## Choosing K:\n",
        "- **K=1**: Just copy nearest neighbor (overfitting!)\n",
        "- **K=5**: More stable, less overfitting\n",
        "- **K=100**: Too smooth, underfitting\n",
        "- **Rule of thumb**: K = ‚àö(number of samples)\n",
        "\n",
        "## Advantages:\n",
        "- ‚úÖ Simple to understand\n",
        "- ‚úÖ No training phase (just store data)\n",
        "- ‚úÖ Can handle complex decision boundaries\n",
        "- ‚úÖ Naturally handles multi-class problems\n",
        "\n",
        "## Disadvantages:\n",
        "- ‚ùå Slow prediction (must compare to all training points)\n",
        "- ‚ùå Needs feature scaling (distances must be comparable)\n",
        "- ‚ùå Sensitive to irrelevant features\n",
        "- ‚ùå Doesn't work well in high dimensions (curse of dimensionality)\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*wW8O-0xVQUFhBGexx2B6hg.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knn",
      "metadata": {
        "id": "knn"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluation-intro",
      "metadata": {
        "id": "evaluation-intro"
      },
      "source": [
        "# Model Evaluation: Beyond Accuracy\n",
        "\n",
        "## Why Accuracy Isn't Enough\n",
        "\n",
        "Imagine a model that predicts **everyone has heart disease**:\n",
        "- If 55% of patients have heart disease ‚Üí **55% accuracy!**\n",
        "- But it's completely useless!\n",
        "\n",
        "We need more metrics to truly understand performance.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-matrix-intro",
      "metadata": {
        "id": "confusion-matrix-intro"
      },
      "source": [
        "## Confusion Matrix: The Foundation\n",
        "\n",
        "A confusion matrix shows 4 types of predictions:\n",
        "\n",
        "```\n",
        "                    Predicted\n",
        "                 Healthy  Disease\n",
        "Actual  Healthy    TN       FP      \n",
        "        Disease    FN       TP\n",
        "```\n",
        "\n",
        "- **True Positive (TP)**: Correctly predicted disease ‚úÖ\n",
        "- **True Negative (TN)**: Correctly predicted healthy ‚úÖ\n",
        "- **False Positive (FP)**: Predicted disease, actually healthy ‚ùå (Type 1 Error)\n",
        "- **False Negative (FN)**: Predicted healthy, actually disease ‚ùå (Type 2 Error)\n",
        "\n",
        "### Medical Context:\n",
        "- **FP (False Alarm)**: Tell healthy person they're sick ‚Üí unnecessary worry/tests\n",
        "- **FN (Missed Disease)**: Tell sick person they're healthy ‚Üí dangerous!\n",
        "\n",
        "In medical diagnosis, **False Negatives are usually worse** than False Positives!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "metrics-explained",
      "metadata": {
        "id": "metrics-explained"
      },
      "source": [
        "## Key Metrics Explained\n",
        "\n",
        "### 1. Accuracy\n",
        "**\"Overall, how often is the model correct?\"**\n",
        "```\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "```\n",
        "- Good when classes are balanced\n",
        "- Misleading with imbalanced classes\n",
        "\n",
        "### 2. Precision\n",
        "**\"When it predicts disease, how often is it right?\"**\n",
        "```\n",
        "Precision = TP / (TP + FP)\n",
        "```\n",
        "- Important when False Positives are costly\n",
        "- Example: Avoid unnecessary surgeries\n",
        "\n",
        "### 3. Recall (Sensitivity)\n",
        "**\"Of all actual disease cases, how many did we catch?\"**\n",
        "```\n",
        "Recall = TP / (TP + FN)\n",
        "```\n",
        "- Important when False Negatives are dangerous\n",
        "- Example: Don't miss cancer patients\n",
        "\n",
        "### 4. F1-Score\n",
        "**\"Balanced measure of Precision and Recall\"**\n",
        "```\n",
        "F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
        "```\n",
        "- Harmonic mean of Precision and Recall\n",
        "- Good single metric when you need balance\n",
        "\n",
        "### Trade-off Example:\n",
        "```\n",
        "Model A: High Precision (90%), Low Recall (50%)\n",
        "‚Üí When it says disease, it's usually right, but misses many cases\n",
        "\n",
        "Model B: Low Precision (60%), High Recall (95%)\n",
        "‚Üí Catches almost all cases, but many false alarms\n",
        "\n",
        "Which is better? Depends on the cost of errors!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluation-function",
      "metadata": {
        "id": "evaluation-function"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(name, y_true, y_pred, y_pred_proba=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a classification model\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  {name} - Detailed Evaluation\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    # Print metrics\n",
        "    print(\"Metrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"  Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
        "    print(f\"  Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\\n\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(f\"                Predicted\")\n",
        "    print(f\"              Healthy  Disease\")\n",
        "    print(f\"Actual Healthy   {cm[0,0]:3d}     {cm[0,1]:3d}\")\n",
        "    print(f\"       Disease   {cm[1,0]:3d}     {cm[1,1]:3d}\\n\")\n",
        "\n",
        "    # Interpretation\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    print(f\"True Negatives (TN):  {tn} - Correctly predicted healthy\")\n",
        "    print(f\"False Positives (FP): {fp} - Healthy predicted as disease\")\n",
        "    print(f\"False Negatives (FN): {fn} - Disease predicted as healthy (DANGEROUS!)\")\n",
        "    print(f\"True Positives (TP):  {tp} - Correctly predicted disease\\n\")\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Healthy', 'Disease']))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "evaluate-all",
      "metadata": {
        "id": "evaluate-all"
      },
      "source": [
        "## Evaluating All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluate-models",
      "metadata": {
        "id": "evaluate-models"
      },
      "outputs": [],
      "source": [
        "# Store results for comparison\n",
        "results = {}\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "results['Logistic Regression'] = evaluate_model(\n",
        "    'Logistic Regression',\n",
        "    y_test_onehot,\n",
        "    y_pred_log,\n",
        "    y_pred_proba_log\n",
        ")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Evaluate Decision Tree\n",
        "\n",
        "# Evaluate Random Forest\n",
        "\n",
        "# Evaluate Naive Bayes\n",
        "\n",
        "# Evaluate SVM\n",
        "\n",
        "# Evaluate KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "comparison-section",
      "metadata": {
        "id": "comparison-section"
      },
      "source": [
        "# Model Comparison\n",
        "\n",
        "Let's compare all models side by side to find our best performer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-table",
      "metadata": {
        "id": "comparison-table"
      },
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
        "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
        "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['f1'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "# Sort by F1-Score (balanced metric)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"                         MODEL COMPARISON\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison-visualization",
      "metadata": {
        "id": "comparison-visualization"
      },
      "outputs": [],
      "source": [
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
        "\n",
        "for idx, (ax, metric, color) in enumerate(zip(axes.flat, metrics, colors)):\n",
        "    data = comparison_df.sort_values(metric, ascending=True)\n",
        "    ax.barh(data['Model'], data[metric], color=color, alpha=0.7)\n",
        "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(data[metric]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interpretation-comparison",
      "metadata": {
        "id": "interpretation-comparison"
      },
      "source": [
        "**Your Interpretation Here:**\n",
        "\n",
        "Which model performed best? Why? What metrics did you prioritize and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confusion-matrix-viz",
      "metadata": {
        "id": "confusion-matrix-viz"
      },
      "source": [
        "# Visualizing Confusion Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-confusion-matrices",
      "metadata": {
        "id": "plot-confusion-matrices"
      },
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (model_name, result) in enumerate(results.items()):\n",
        "    cm = result['confusion_matrix']\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Healthy', 'Disease'],\n",
        "                yticklabels=['Healthy', 'Disease'],\n",
        "                ax=axes[idx], cbar=False)\n",
        "\n",
        "    axes[idx].set_title(f'{model_name}\\nF1: {result[\"f1\"]:.3f}',\n",
        "                        fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual', fontsize=10)\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance-intro",
      "metadata": {
        "id": "feature-importance-intro"
      },
      "source": [
        "# Feature Importance: What Matters Most?\n",
        "\n",
        "Tree-based models (Decision Tree, Random Forest) can tell us **which features** are most important for predictions!\n",
        "\n",
        "## How it works:\n",
        "- Measures how much each feature **reduces uncertainty** (impurity)\n",
        "- Higher importance = feature is used more often for splitting\n",
        "- Helps understand what the model learned\n",
        "\n",
        "## Why it's useful:\n",
        "- ‚úÖ Understand model decisions\n",
        "- ‚úÖ Identify key risk factors  \n",
        "- ‚úÖ Feature selection (remove unimportant features)\n",
        "- ‚úÖ Medical insights (what causes heart disease?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feature-importance",
      "metadata": {
        "id": "feature-importance"
      },
      "outputs": [],
      "source": [
        "# Get feature importance from Random Forest (usually most reliable)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_train_label.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance (Random Forest):\")\n",
        "print(\"=\"*50)\n",
        "print(feature_importance.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'],\n",
        "         color='steelblue', alpha=0.7)\n",
        "plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feature-importance-interpretation",
      "metadata": {
        "id": "feature-importance-interpretation"
      },
      "source": [
        "**Your Interpretation Here:**\n",
        "\n",
        "Which features are most important? Does this align with medical knowledge? Are there any surprises?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cross-validation-intro",
      "metadata": {
        "id": "cross-validation-intro"
      },
      "source": [
        "# Cross-Validation: More Robust Evaluation\n",
        "\n",
        "## The Problem with Single Train-Test Split:\n",
        "- Results depend on **which samples** ended up in test set\n",
        "- Lucky/unlucky split can make models look better/worse\n",
        "- Not using all data for training AND testing\n",
        "\n",
        "## Cross-Validation Solution:\n",
        "\n",
        "**K-Fold Cross-Validation** (typically K=5 or K=10):\n",
        "\n",
        "```\n",
        "Fold 1: [Test] [Train] [Train] [Train] [Train]\n",
        "Fold 2: [Train] [Test] [Train] [Train] [Train]\n",
        "Fold 3: [Train] [Train] [Test] [Train] [Train]\n",
        "Fold 4: [Train] [Train] [Train] [Test] [Train]\n",
        "Fold 5: [Train] [Train] [Train] [Train] [Test]\n",
        "\n",
        "Final Score = Average of all 5 folds\n",
        "```\n",
        "\n",
        "## Benefits:\n",
        "- ‚úÖ More reliable estimate of performance\n",
        "- ‚úÖ Uses all data for both training and testing\n",
        "- ‚úÖ Reduces variance in results\n",
        "- ‚úÖ Shows standard deviation (how stable is the model?)\n",
        "\n",
        "## Trade-off:\n",
        "- Takes K times longer (trains K models instead of 1)\n",
        "- But gives much more confidence in results!\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*rgba1BIOUys7wQcXcL4U5A.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cross-validation",
      "metadata": {
        "id": "cross-validation"
      },
      "outputs": [],
      "source": [
        "# Perform 5-fold cross-validation on our best models\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"                    5-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "log_scores = cross_val_score(log_reg, X_train_scaled, y_train_onehot, cv=5, scoring='f1')\n",
        "cv_results['Logistic Regression'] = log_scores\n",
        "print(f\"Logistic Regression:\")\n",
        "print(f\"  Scores: {log_scores}\")\n",
        "print(f\"  Mean F1: {log_scores.mean():.4f} (+/- {log_scores.std() * 2:.4f})\\n\")\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# SVM\n",
        "\n",
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Set, Validation Set, Testing Set**\n",
        "\n",
        "When we build machine learning models, we do not use all the data in the same way.  \n",
        "We split the data so that learning, model selection, and final evaluation are kept separate.  \n",
        "Each split has a specific role and answers a different question.\n",
        "\n",
        "- The **training set** is used to learn the model parameters.\n",
        "- The **validation set** is used to compare models or hyperparameters and decide which one is better.\n",
        "- The **test set** is used only once, at the end, to estimate how well the chosen model generalizes to unseen data.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/1*OECM6SWmlhVzebmSuvMtBg.png\">"
      ],
      "metadata": {
        "id": "E4zo_EkbkAT1"
      },
      "id": "E4zo_EkbkAT1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning: Optimizing Performance\n",
        "\n",
        "## What are Hyperparameters?\n",
        "\n",
        "Settings you choose **before** training that control how the model learns:\n",
        "\n",
        "### Random Forest Hyperparameters:\n",
        "- `n_estimators`: Number of trees (more = better, but slower)\n",
        "- `max_depth`: How deep each tree can grow\n",
        "- `min_samples_split`: Minimum samples needed to split a node\n",
        "- `min_samples_leaf`: Minimum samples in a leaf node\n",
        "\n",
        "### KNN Hyperparameters:\n",
        "- `n_neighbors`: Number of neighbors (K value)\n",
        "- `weights`: All equal or weighted by distance?\n",
        "- `metric`: Distance metric to use\n",
        "\n",
        "### SVM Hyperparameters:\n",
        "- `C`: Regularization (higher = more complex)\n",
        "- `gamma`: Kernel coefficient (how far influence reaches)\n",
        "- `kernel`: Type of decision boundary (linear, rbf, poly)\n",
        "\n",
        "## Grid Search:\n",
        "Try **all combinations** of hyperparameters and pick the best!\n",
        "\n",
        "```python\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "# Tests: 3 √ó 3 = 9 combinations\n",
        "```\n",
        "\n",
        "## Warning:\n",
        "- Can be SLOW with many parameters\n",
        "- Use small grid for learning\n",
        "- Cross-validation inside makes it even slower!"
      ],
      "metadata": {
        "id": "EX32_zDikWA1"
      },
      "id": "EX32_zDikWA1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What happens in this GridSearchCV example (mental model)\n",
        "\n",
        "- Start with a **train / test split**\n",
        "  - `X_train_label, y_train_label` ‚Üí used for tuning and training\n",
        "  - `X_test_label, y_test_label` ‚Üí kept untouched until the end\n",
        "\n",
        "- Define a **hyperparameter grid**\n",
        "  - 108 different Random Forest configurations\n",
        "  - Each configuration is a different ‚Äúmodel recipe‚Äù\n",
        "\n",
        "- Call `grid_search.fit(X_train_label, y_train_label)`\n",
        "  - GridSearchCV only sees the **training set**\n",
        "\n",
        "- Inside GridSearchCV:\n",
        "  - Split the training set into **5 CV folds**\n",
        "  - For each hyperparameter combination:\n",
        "    - Train on 4 folds\n",
        "    - Validate on 1 fold\n",
        "    - Repeat until each fold has been validation once\n",
        "    - Average the 5 F1 scores ‚Üí one CV score per combination\n",
        "\n",
        "- Compare all combinations\n",
        "  - Select the one with the **highest mean CV F1**\n",
        "\n",
        "- **Refit step (automatic)**\n",
        "  - Take the best hyperparameters\n",
        "  - Train one final model on **100% of the training data**\n",
        "  - Store it as `best_estimator_`\n",
        "\n",
        "- Final evaluation\n",
        "  - Use `best_estimator_` to predict on the **test set**\n",
        "  - Compute test F1 once\n",
        "  - This is the honest performance estimate\n",
        "\n",
        "- Key takeaway\n",
        "  - CV folds are **validation**, not the real test\n",
        "  - The test set is used **only once**, at the very end\n"
      ],
      "metadata": {
        "id": "pJjuCd8xkRHb"
      },
      "id": "pJjuCd8xkRHb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for Random Forest\n",
        "print(\"Hyperparameter Tuning for Random Forest...\\n\")\n",
        "\n",
        "# Define parameter grid (keeping it small for speed)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                    # 5-fold cross-validation\n",
        "    scoring='f1',            # Optimize for F1-score\n",
        "    n_jobs=-1,               # Use all CPU cores\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit (this might take a while!)\n",
        "print(f\"Testing {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['min_samples_split']) * len(param_grid['min_samples_leaf'])} combinations...\")\n",
        "grid_search.fit(X_train_label, y_train_label)\n",
        "\n",
        "# Best parameters\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BEST PARAMETERS FOUND:\")\n",
        "print(\"=\"*60)\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest F1-Score (Cross-Validation): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Test on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_best = best_rf.predict(X_test_label)\n",
        "test_f1 = f1_score(y_test_label, y_pred_best)\n",
        "print(f\"Test Set F1-Score: {test_f1:.4f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "h7hk6l29ka9E"
      },
      "id": "h7hk6l29ka9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Your Task:\n",
        "Perform Hyperparameter Tuning using GridSearchCV for your **TOP 3 Models** so far (the ones with better score from simple CV you performed above)\n",
        "\n",
        "- Use Hypeparameters mentioned above for RANDOM FOREST, SVM, KNN. If you have\n",
        "another model in the top 3 perform the relevant research on the most important\n",
        "hyperparameters needing tuning. You **don't have to have** 108 combinations for every model, you might just have 20 or less, do not worry about that.\n"
      ],
      "metadata": {
        "id": "oqo60LzWkdAk"
      },
      "id": "oqo60LzWkdAk"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for 2nd Model\n",
        "print(\"Hyperparameter Tuning for SVM...\\n\")\n",
        "\n",
        "# Define parameter grid for 2nd Model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Fit (this might take a while!)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Best parameters\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Test on test set\n",
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "7RACUdoikckZ"
      },
      "id": "7RACUdoikckZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for 3rd Model\n",
        "print(\"Hyperparameter Tuning for SVM...\\n\")\n",
        "\n",
        "# Define parameter grid for 3rd Model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Fit (this might take a while!)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Best parameters\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Test on test set\n",
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "De1ODO8zkf3u"
      },
      "id": "De1ODO8zkf3u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **YOUR INTERPRETATION HERE**"
      ],
      "metadata": {
        "id": "naILPncRkk8Q"
      },
      "id": "naILPncRkk8Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Model Selection & Insights\n",
        "\n",
        "## üìä Dataset Information\n",
        "- **Total Samples**:\n",
        "- **Training Samples**:\n",
        "- **Test Samples**:\n",
        "- **Number of Features**:\n",
        "\n",
        "## üèÜ Best Performing Model\n",
        "- **Model**:\n",
        "- **F1-Score**: (OR THE METRIC YOU USED)\n",
        "\n",
        "## üìà Top 3 Most Important Features\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n",
        "## üí° Key Insights\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "4.\n",
        "\n",
        "## üéØ Recommendations\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "\n",
        "## üî≠ How could/would you improve this Project ?\n",
        "-\n",
        "-\n",
        "-\n",
        "-\n",
        "\n",
        "## üèÅ Final Comment on the Project\n",
        "-"
      ],
      "metadata": {
        "id": "jRJfrplKkndm"
      },
      "id": "jRJfrplKkndm"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}